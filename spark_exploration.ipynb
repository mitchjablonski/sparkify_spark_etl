{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "import posixpath\n",
    "\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "input_data = \"s3a://udacity-dend\"\n",
    "output_data = \"s3a://sparkify-data-processed\"\n",
    "song_data = posixpath.join(input_data, 'song_data/A/B/C/*.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Explore Song Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.json(song_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[artist_id: string, artist_latitude: double, artist_location: string, artist_longitude: double, artist_name: string, duration: double, num_songs: bigint, song_id: string, title: string, year: bigint]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[song_id: string, title: string, artist_id: string, year: bigint, duration: double]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('song_id', 'title', 'artist_id', 'year', 'duration').filter(col('song_id').isNotNull()).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[artist_id: string, artist_name: string, artist_location: string, artist_latitude: double, artist_longitude: double]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('artist_id', 'artist_name', 'artist_location', 'artist_latitude', 'artist_longitude').filter(col('artist_id').isNotNull()).drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Test the ETL Pipeline code on our sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "songs_table = df.select('song_id', 'title', 'artist_id', 'year', 'duration').filter(col('song_id').isNotNull()).drop_duplicates()\n",
    "\n",
    "# write songs table to parquet files partitioned by year and artist\n",
    "songs_table.write.parquet(posixpath.join(output_data, \"songs/\"), mode=\"overwrite\", partitionBy=[\"year\",\"artist_id\"])\n",
    "\n",
    "# extract columns to create artists table\n",
    "artists_table = df.select('artist_id', 'artist_name', 'artist_location', 'artist_latitude', 'artist_longitude').filter(col('artist_id').isNotNull()).drop_duplicates()\n",
    "\n",
    "# write artists table to parquet files\n",
    "artists_table.write.parquet(posixpath.join(output_data, \"artists/\"), mode=\"overwrite\", partitionBy=[\"artist_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Explore Log Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "log_data =posixpath.join(input_data, 'log_data/2018/11/*.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_log = spark.read.json(log_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[artist: string, auth: string, firstName: string, gender: string, itemInSession: bigint, lastName: string, length: double, level: string, location: string, method: string, page: string, registration: double, sessionId: bigint, song: string, status: bigint, ts: bigint, userAgent: string, userId: string]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|           ts|\n",
      "+-------------+\n",
      "|1542241826796|\n",
      "|1542242481796|\n",
      "|1542242741796|\n",
      "|1542247071796|\n",
      "|1542252577796|\n",
      "|1542253449796|\n",
      "|1542253460796|\n",
      "|1542260074796|\n",
      "|1542260277796|\n",
      "|1542260935796|\n",
      "|1542261224796|\n",
      "|1542261356796|\n",
      "|1542261662796|\n",
      "|1542261713796|\n",
      "|1542262057796|\n",
      "|1542262233796|\n",
      "|1542262434796|\n",
      "|1542262456796|\n",
      "|1542262679796|\n",
      "|1542262728796|\n",
      "+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_log.select('ts').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "df_log = df_log.withColumn('start_time', (df_log.ts/1000).cast(dataType=TimestampType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[start_time: timestamp, hour: int, day: int, week: int, month: int, year: int, weekday: int]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format, dayofweek\n",
    "\n",
    "df_log.withColumn(\"hour\",hour(\"start_time\"))\\\n",
    "      .withColumn(\"day\",dayofmonth(\"start_time\"))\\\n",
    "      .withColumn(\"week\",weekofyear(\"start_time\"))\\\n",
    "      .withColumn(\"month\",month(\"start_time\"))\\\n",
    "      .withColumn(\"year\",year(\"start_time\"))\\\n",
    "      .withColumn(\"weekday\",dayofweek(\"start_time\"))\\\n",
    "      .select(\"start_time\",\"hour\", \"day\", \"week\", \"month\", \"year\", \"weekday\").drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Test the ETL Pipeline code on a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read log data file\n",
    "df = df_log\n",
    "\n",
    "# filter by actions for song plays\n",
    "df = df.filter(col('page') == 'NextSong')\n",
    "\n",
    "# extract columns for users table    \n",
    "users_table = df.select(\"userId\",\"firstName\",\"lastName\",\"gender\",\"level\").filter(col('userId').isNotNull()).drop_duplicates()\n",
    "\n",
    "# write users table to parquet files\n",
    "users_table.write.parquet(posixpath.join(output_data, \"users/\"), mode=\"overwrite\")\n",
    "\n",
    "# create timestamp column from original timestamp column\n",
    "df = df.withColumn('start_time', (df.ts/1000).cast(dataType=TimestampType()))\n",
    "\n",
    "# extract columns to create time table\n",
    "time_table = df_log.withColumn(\"hour\",hour(\"start_time\"))\\\n",
    "                   .withColumn(\"day\",dayofmonth(\"start_time\"))\\\n",
    "                   .withColumn(\"week\",weekofyear(\"start_time\"))\\\n",
    "                   .withColumn(\"month\",month(\"start_time\"))\\\n",
    "                   .withColumn(\"year\",year(\"start_time\"))\\\n",
    "                   .withColumn(\"weekday\",dayofweek(\"start_time\"))\\\n",
    "                   .select(\"start_time\",\"hour\", \"day\", \"week\", \"month\", \"year\", \"weekday\").drop_duplicates()\n",
    "\n",
    "# write time table to parquet files partitioned by year and month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# write time table to parquet files partitioned by year and month\n",
    "time_table.write.parquet(posixpath.join(output_data, \"time_table/\"), mode=\"overwrite\", partitionBy=[\"year\", \"month\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read in song data to use for songplays table\n",
    "song_df =  spark.read.parquet(posixpath.join(output_data,'songs_table/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[start_time: timestamp, user_id: string, level: string, song_id: string, artist_id: string, session_id: bigint, location: string, user_agent: string, songplay_id: bigint]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songplays_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col, monotonically_increasing_id\n",
    "# extract columns from joined song and log datasets to create songplays table \n",
    "songplays_table = df.join(song_df, df.song == song_df.title, how='inner')\\\n",
    "                    .select(col(\"start_time\"),col(\"userId\").alias(\"user_id\"),\n",
    "                            \"level\",\"song_id\",\"artist_id\", col(\"sessionId\").alias(\"session_id\"), \"location\", col(\"userAgent\").alias(\"user_agent\")).drop_duplicates()\n",
    "songplays_table = songplays_table.withColumn('songplay_id', monotonically_increasing_id())\n",
    "\n",
    "# write songplays table to parquet files partitioned by year and month\n",
    "songplays_table.write.parquet(posixpath.join(output_data, \"songplays_table/\"), mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
